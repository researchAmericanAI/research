# researchAmericanAI

/endpoint → endpoint entry screen (type in the endpoint where Ollama is running)

/model → model name entry screen (type in the name of the model from ollama library)

/release → export current conversation to your browser donloads; drag and drop them into the window to use them later

/recover → deletes the last dropped file or the last response from the model, whichever came last - 0: yes, 1: no

/polarity → 0: answers with the full 55 tool set; 1: answers yes, no, or directly

*support re:search https://ko-fi.com/researchkofi*

*prerequisites: python 3.6+. Ollama* 

*************
installation:
*************

download the repository

Mac:

press command + spacebar and type terminal 

use enter on the keyboard or select the icon

enter the following:

cd ~/Downloads/research-main *(the location where you downloaded the repository may differ)*

chmod +x start.command *(allows the start.command on your Mac)*

./start.command *(opens index.html, the UI. clicking index in the folder will reopen the UI if you lose it.)*

*leave the terminal running...*

done

*after the first time you run the chmod +x start.command, the server can simply be ran by executing the start.command from the root folder in an empty terminal window*

PC: 

start.bat included

i couldn't test

if 'one clicking' start.bat fails

files may run separately 

i'm awaiting feedback

i hope it works and doesn't cause any trouble

enjoy!
